{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Importing evertything "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#importing part\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms.functional as fn   \n",
    "import torch\n",
    "from scipy.stats import entropy\n",
    "\n",
    "from torchvision.utils import save_image \n",
    "\n",
    "\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch.nn as n\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Setting dataset folder and Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONSTANTS\n",
    "BATCH_SIZE = 128\n",
    "IMG_CHANNELS=3\n",
    "NUM_EPOCHS=2\n",
    "Z_DIM=100\n",
    "LR = 2e-4\n",
    "IMAGE_SIZE = 64\n",
    "FEATURES_GEN =64\n",
    "FEATURES_DISC=64\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "folder_path = '/home/metaphysicist/Coding/Research/Font_Generation/data/Images/Images_dir/a/'\n",
    "#folder_path = '/home/metaphysicist/Coding/Research/Font_Generation/Georgian_dataset/data/'\n",
    "image_shape = (32,32,3) # \n",
    "nm_imgs = np.sort(os.listdir(folder_path)) # sorting files in folder\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.1 some options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A']\n"
     ]
    }
   ],
   "source": [
    "# Experiments: print specific letter folder\n",
    "print(nm_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize = transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "\n",
    "Transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(IMAGE_SIZE),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            [0.5 for _ in range(IMG_CHANNELS)], [0.5 for _ in range(IMG_CHANNELS)]\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "Dataset = datasets.ImageFolder(root=folder_path,transform=Transforms)\n",
    "\n",
    "Dataset_Loader = DataLoader(Dataset,batch_size=BATCH_SIZE,shuffle=True) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Transforming, Getting And Visualising each photo and their parameters  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* For running data images plotting process uncomment plot_img function call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_img(loader):\n",
    "    for batch_idx, (real, i) in enumerate(loader):\n",
    "\n",
    "\n",
    "        \n",
    "        # print(batch_idx, (real, i))\n",
    "        # imgTensor, labels = next(loader_iter)\n",
    "\n",
    "        img_grid_real = torchvision.utils.make_grid(\n",
    "                            real[:1], normalize=True\n",
    "                        )\n",
    "\n",
    "        img_grid_real = img_grid_real.permute(1, 2, 0)\n",
    "        # Set up plot config\n",
    "        plt.figure(figsize=(8, 2), dpi=300)\n",
    "        plt.axis('off')\n",
    "\n",
    "        # Plot Image Grid\n",
    "        plt.imshow(img_grid_real)\n",
    "\n",
    "\n",
    "        # Show the plot\n",
    "        plt.show()\n",
    "\n",
    "# plot_img(Dataset_Loader)\n",
    "##### For running data images plotting process uncomment plot_img function call [upper]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Building Models And Setting Weight Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(n.Module):\n",
    "    def __init__(self,channels_img,features_dim):\n",
    "        super(Discriminator,self).__init__()\n",
    "        self.disc = n.Sequential(\n",
    "            # Input is N x channels_img x 64 x 64\n",
    "            n.Conv2d(in_channels=channels_img, out_channels=features_dim, kernel_size=4,stride=2,padding=1),\n",
    "            # 32 x 32 \n",
    "            n.LeakyReLU(0.2),\n",
    "            \n",
    "            self._block(features_dim, features_dim*2, 4, 2, 1),\n",
    "            # 16 x 16\n",
    "            self._block(features_dim*2, features_dim*4, 4, 2, 1),\n",
    "            # 8 x 8\n",
    "            self._block(features_dim*4, features_dim*8, 4, 2, 1),\n",
    "            # # 4 x 4\n",
    "            n.Conv2d(features_dim*8, 1, kernel_size=4, stride=2, padding=0),\n",
    "            # 1 x 1\n",
    "            n.Sigmoid(),\n",
    "        )\n",
    "    \n",
    "    def _block(self,input_features,output_features,kernel,stride,padding):\n",
    "        return n.Sequential(\n",
    "            n.Conv2d(input_features,output_features,kernel,stride,padding,bias=False),\n",
    "            n.BatchNorm2d(output_features),\n",
    "            n.LeakyReLU(0.2),\n",
    "        )\n",
    "    \n",
    "    def forward(self,x):\n",
    "        return self.disc(x)\n",
    "\n",
    "\n",
    "class Generator(n.Module):\n",
    "    def __init__(self,z_dim,channels_img,features_g):\n",
    "        super(Generator,self).__init__()\n",
    "        self.gen = n.Sequential(\n",
    "            # Input N x z_dim x 1 x 1\n",
    "            self._block(z_dim, features_g*16, kernel=4, stride=1, padding=0),\n",
    "            # N x f_g*16 x 4 x 4\n",
    "            self._block(features_g*16, features_g*8, kernel=4, stride=2, padding=1),\n",
    "            # 8x8\n",
    "            self._block(features_g*8,features_g*4,kernel=4,stride=2,padding=1),\n",
    "            # 4x4\n",
    "            self._block(features_g*4, features_g*2, kernel=4, stride=2, padding=1),\n",
    "            # # 2x2\n",
    "            n.ConvTranspose2d(features_g*2, channels_img, kernel_size=4, stride=2, padding=1),\n",
    "            n.Tanh()\n",
    "        )\n",
    "    \n",
    "    def _block(self,input_channels,output_channels,kernel,stride,padding):\n",
    "        return n.Sequential(\n",
    "            n.ConvTranspose2d(input_channels, output_channels,kernel, stride, padding),\n",
    "            n.BatchNorm2d(output_channels),\n",
    "            n.ReLU()\n",
    "        )\n",
    "    \n",
    "    def forward(self,x):\n",
    "        return self.gen(x)\n",
    "\n",
    "def initialize_weights(model):\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, (n.Conv2d,n.BatchNorm2d,n.ConvTranspose2d)):\n",
    "            n.init.normal_(m.weight.data,0.0,0.02)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Training and Implementing PART I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discriminator(\n",
       "  (disc): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (1): LeakyReLU(negative_slope=0.2)\n",
       "    (2): Sequential(\n",
       "      (0): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.2)\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.2)\n",
       "    )\n",
       "    (4): Sequential(\n",
       "      (0): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.2)\n",
       "    )\n",
       "    (5): Conv2d(512, 1, kernel_size=(4, 4), stride=(2, 2))\n",
       "    (6): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen = Generator(Z_DIM,IMG_CHANNELS,FEATURES_GEN).to(device)\n",
    "disc = Discriminator(IMG_CHANNELS,FEATURES_DISC).to(device)\n",
    "initialize_weights(gen)\n",
    "initialize_weights(disc)\n",
    "opt_gen = torch.optim.Adam(gen.parameters(),lr=LR,betas=(0.5,0.999))\n",
    "opt_disc = torch.optim.Adam(disc.parameters(),lr=LR,betas=(0.5,0.999))\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "fixed_noise = torch.randn(BATCH_SIZE,Z_DIM,1,1).to(device)\n",
    "writer_real = SummaryWriter(\"logs/real\")\n",
    "writer_fake = SummaryWriter(\"logs/fake\")\n",
    "gen.train()\n",
    "disc.train()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INCEPTION SCORE START"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# import torch\n",
    "# from torch import nn\n",
    "# import numpy as np\n",
    "# from scipy.stats import entropy\n",
    "\n",
    "# def inception_score(real_images, fake_images, inception_model, batch_size=64, num_splits=10):\n",
    "#     \"\"\"\n",
    "#     Calculates the Inception Score (IS) to evaluate the quality of generated images.\n",
    "\n",
    "#     Args:\n",
    "#         real_images (torch.Tensor): Real images as a tensor.\n",
    "#         fake_images (torch.Tensor): Fake/generated images as a tensor.\n",
    "#         inception_model (torch.nn.Module): Pre-trained Inception model.\n",
    "#         batch_size (int): Batch size for feeding images to the Inception model.\n",
    "#         num_splits (int): Number of splits to calculate the IS. More splits yield a more accurate score.\n",
    "\n",
    "#     Returns:\n",
    "#         float: Inception Score.\n",
    "#     \"\"\"\n",
    "\n",
    "#     # Set device (GPU if available, else CPU)\n",
    "#     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#     print(device)\n",
    "#     # Set the model to evaluation mode\n",
    "#     inception_model.eval()\n",
    "\n",
    "#     # Calculate the number of samples\n",
    "#     num_samples = real_images.size(0)\n",
    "\n",
    "#     # Split the images into batches\n",
    "#     real_batches = torch.split(real_images, batch_size)\n",
    "#     fake_batches = torch.split(fake_images, batch_size)\n",
    "\n",
    "#     # Initialize the list to store the scores for each split\n",
    "#     split_scores = []\n",
    "\n",
    "#     # Calculate the Inception Score for each split\n",
    "#     for i in range(num_splits):\n",
    "#         # Get a batch of real and fake images\n",
    "#         real_batch = real_batches[i].to(device)\n",
    "#         fake_batch = fake_batches[i].to(device)\n",
    "\n",
    "#         # Compute the logits for real and fake images\n",
    "        \n",
    "#         with torch.no_grad():\n",
    "#             real_logits = inception_model(real_batch).to(device)\n",
    "#             fake_logits = inception_model(fake_batch).to(device)\n",
    "\n",
    "#         # Compute the softmax probabilities for real and fake images\n",
    "#         real_probs = torch.softmax(real_logits, dim=1)\n",
    "#         fake_probs = torch.softmax(fake_logits, dim=1)\n",
    "\n",
    "#         # Compute the marginal entropy of the softmax probabilities\n",
    "#         real_entropy = torch.sum(-real_probs * torch.log(real_probs + 1e-8), dim=1)\n",
    "#         fake_entropy = torch.sum(-fake_probs * torch.log(fake_probs + 1e-8), dim=1)\n",
    "\n",
    "#         # Compute the average entropy for each batch\n",
    "#         avg_real_entropy = torch.mean(real_entropy)\n",
    "#         avg_fake_entropy = torch.mean(fake_entropy)\n",
    "\n",
    "#         # Calculate the Inception Score for the current split\n",
    "#         split_score = torch.exp(avg_real_entropy - avg_fake_entropy)\n",
    "#         split_scores.append(split_score.item())\n",
    "\n",
    "#     # Calculate the final Inception Score as the exponential of the mean of the split scores\n",
    "#     is_mean = np.mean(split_scores)\n",
    "#     is_std = np.std(split_scores)\n",
    "#     inception_score = is_mean\n",
    "\n",
    "#     return inception_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from scipy.linalg import sqrtm\n",
    "\n",
    "def calculate_inception_score_and_fid(generated_images, real_images):\n",
    "\n",
    "    inception_model = torchvision.models.inception_v3(pretrained=True, transform_input=False, aux_logits=True)\n",
    "    inception_model = inception_model.eval()\n",
    "\n",
    "    # Preprocess the generated images\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((299, 299)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Standard Inception normalization\n",
    "    ])\n",
    "    generated_images = torch.stack([preprocess(image) for image in generated_images])\n",
    "    \n",
    "    # Get the predictions for the generated images\n",
    "    predictions = inception_model(generated_images)\n",
    "\n",
    "    # Calculate the IS\n",
    "    probabilities = torch.nn.functional.softmax(predictions, dim=1)\n",
    "    entropy = torch.sum(probabilities * torch.log(probabilities), dim=1)\n",
    "    inception_score = torch.exp(torch.mean(entropy))\n",
    "\n",
    "    # Preprocess the real images\n",
    "    real_images = torch.stack([preprocess(image) for image in real_images])\n",
    "\n",
    "    # Extract the feature representations from the Inception model\n",
    "    features_real = inception_model(real_images).detach().numpy()\n",
    "    features_generated = predictions.detach().numpy()\n",
    "\n",
    "    # Calculate the mean and covariance for real and generated features\n",
    "    mean_real = np.mean(features_real, axis=0)\n",
    "    mean_generated = np.mean(features_generated, axis=0)\n",
    "    cov_real = np.cov(features_real, rowvar=False)\n",
    "    cov_generated = np.cov(features_generated, rowvar=False)\n",
    "\n",
    "    # Calculate the FID using the Fr√©chet distance\n",
    "    diff = mean_real - mean_generated\n",
    "    sqrt_cov_product = sqrtm(cov_real.dot(cov_generated))\n",
    "    fid = np.real(diff.dot(diff) + np.trace(cov_real + cov_generated - 2 * sqrt_cov_product))\n",
    "\n",
    "    return fid.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inception_model = torchvision.models.inception_v3(pretrained=True, transform_input=False, aux_logits=True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INCEPTION SCORE FINISH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/20] Batch 0/118                             Loss D: 0.7004, loss G: 0.7887\n",
      "1861.990803568836\n",
      "Freschet Inception Distance less than 1000\n",
      "Epoch [0/20] Batch 100/118                             Loss D: 0.0144, loss G: 4.1529\n",
      "1503.157092744716\n",
      "Freschet Inception Distance less than 1000\n",
      "Epoch [1/20] Batch 0/118                             Loss D: 0.0108, loss G: 4.4544\n",
      "1521.7171256830648\n",
      "Freschet Inception Distance less than 1000\n",
      "Epoch [1/20] Batch 100/118                             Loss D: 0.7567, loss G: 1.9168\n",
      "1497.6841438778058\n",
      "Freschet Inception Distance less than 1000\n",
      "Epoch [2/20] Batch 0/118                             Loss D: 0.5103, loss G: 1.1008\n",
      "1313.7678065705466\n",
      "Freschet Inception Distance less than 1000\n",
      "Epoch [2/20] Batch 100/118                             Loss D: 0.5871, loss G: 1.5588\n",
      "1264.6165769039003\n",
      "Freschet Inception Distance less than 1000\n",
      "Epoch [3/20] Batch 0/118                             Loss D: 0.5131, loss G: 1.2438\n",
      "1075.358969621932\n",
      "Freschet Inception Distance less than 1000\n",
      "fake 128\n",
      "Epoch [3/20] Batch 100/118                             Loss D: 0.4987, loss G: 2.0902\n",
      "1031.8254881768607\n",
      "Freschet Inception Distance less than 1000\n",
      "fake 128\n",
      "Epoch [4/20] Batch 0/118                             Loss D: 0.7590, loss G: 2.5081\n",
      "1177.1925499965787\n",
      "Freschet Inception Distance less than 1000\n",
      "fake 128\n",
      "Epoch [4/20] Batch 100/118                             Loss D: 0.4416, loss G: 1.4331\n",
      "957.7870245154842\n",
      "Freschet Inception Distance less than 1000\n",
      "fake 128\n",
      "Epoch [5/20] Batch 0/118                             Loss D: 0.5504, loss G: 0.9960\n",
      "989.3692785326971\n",
      "Freschet Inception Distance less than 1000\n",
      "fake 128\n",
      "Epoch [5/20] Batch 100/118                             Loss D: 0.4788, loss G: 1.4160\n",
      "868.8379719335894\n",
      "Freschet Inception Distance less than 1000\n",
      "fake 128\n",
      "Epoch [6/20] Batch 0/118                             Loss D: 0.3808, loss G: 2.0835\n",
      "790.2440873517777\n",
      "Freschet Inception Distance less than 1000\n",
      "fake 128\n",
      "Epoch [6/20] Batch 100/118                             Loss D: 0.2556, loss G: 1.8272\n",
      "696.1239475851803\n",
      "Freschet Inception Distance less than 1000\n",
      "fake 128\n",
      "Epoch [7/20] Batch 0/118                             Loss D: 0.3576, loss G: 1.7608\n",
      "773.0499828953602\n",
      "Freschet Inception Distance less than 1000\n",
      "fake 128\n",
      "Epoch [7/20] Batch 100/118                             Loss D: 0.0779, loss G: 3.0760\n",
      "713.32120933952\n",
      "Freschet Inception Distance less than 1000\n",
      "fake 128\n",
      "Epoch [8/20] Batch 0/118                             Loss D: 0.4960, loss G: 1.4605\n",
      "687.959317666953\n",
      "Freschet Inception Distance less than 1000\n",
      "fake 128\n",
      "Epoch [8/20] Batch 100/118                             Loss D: 0.0256, loss G: 4.4238\n",
      "843.230867845605\n",
      "Freschet Inception Distance less than 1000\n",
      "fake 128\n",
      "Epoch [9/20] Batch 0/118                             Loss D: 0.0116, loss G: 5.7708\n",
      "794.1446574674908\n",
      "Freschet Inception Distance less than 1000\n",
      "fake 128\n",
      "Epoch [9/20] Batch 100/118                             Loss D: 0.0999, loss G: 3.1765\n",
      "690.7010947685368\n",
      "Freschet Inception Distance less than 1000\n",
      "fake 128\n",
      "Epoch [10/20] Batch 0/118                             Loss D: 0.4660, loss G: 1.8890\n",
      "779.5210792910684\n",
      "Freschet Inception Distance less than 1000\n",
      "fake 128\n",
      "Epoch [10/20] Batch 100/118                             Loss D: 0.5154, loss G: 1.1554\n",
      "700.734700751917\n",
      "Freschet Inception Distance less than 1000\n",
      "fake 128\n",
      "Epoch [11/20] Batch 0/118                             Loss D: 0.5177, loss G: 1.2704\n",
      "723.8758795580156\n",
      "Freschet Inception Distance less than 1000\n",
      "fake 128\n",
      "Epoch [11/20] Batch 100/118                             Loss D: 0.0393, loss G: 3.6589\n",
      "637.349631321566\n",
      "Freschet Inception Distance less than 1000\n",
      "fake 128\n",
      "Epoch [12/20] Batch 0/118                             Loss D: 0.0284, loss G: 3.9073\n",
      "665.2869768066267\n",
      "Freschet Inception Distance less than 1000\n",
      "fake 128\n",
      "Epoch [12/20] Batch 100/118                             Loss D: 0.3437, loss G: 1.0343\n",
      "682.7862385146854\n",
      "Freschet Inception Distance less than 1000\n",
      "fake 128\n",
      "Epoch [13/20] Batch 0/118                             Loss D: 0.6547, loss G: 0.6979\n",
      "706.0103708306922\n",
      "Freschet Inception Distance less than 1000\n",
      "fake 128\n",
      "Epoch [13/20] Batch 100/118                             Loss D: 0.0309, loss G: 3.8573\n",
      "869.9423096934985\n",
      "Freschet Inception Distance less than 1000\n",
      "fake 128\n",
      "Epoch [14/20] Batch 0/118                             Loss D: 0.0238, loss G: 4.4392\n",
      "1184.6071746132288\n",
      "Freschet Inception Distance less than 1000\n",
      "fake 128\n",
      "Epoch [14/20] Batch 100/118                             Loss D: 0.0050, loss G: 5.1469\n",
      "1434.6922938189127\n",
      "Freschet Inception Distance less than 1000\n",
      "Epoch [15/20] Batch 0/118                             Loss D: 0.0043, loss G: 5.4068\n",
      "1465.3813768591044\n",
      "Freschet Inception Distance less than 1000\n",
      "Epoch [15/20] Batch 100/118                             Loss D: 0.0021, loss G: 5.9395\n",
      "1585.9703090425514\n",
      "Freschet Inception Distance less than 1000\n",
      "Epoch [16/20] Batch 0/118                             Loss D: 0.0022, loss G: 5.9992\n",
      "1514.313146726299\n",
      "Freschet Inception Distance less than 1000\n",
      "Epoch [16/20] Batch 100/118                             Loss D: 0.5716, loss G: 11.6738\n",
      "1670.3462509776841\n",
      "Freschet Inception Distance less than 1000\n",
      "Epoch [17/20] Batch 0/118                             Loss D: 0.0518, loss G: 4.6049\n",
      "1589.8193288992643\n",
      "Freschet Inception Distance less than 1000\n",
      "Epoch [17/20] Batch 100/118                             Loss D: 0.0289, loss G: 4.0800\n",
      "1167.6945612666877\n",
      "Freschet Inception Distance less than 1000\n",
      "fake 128\n",
      "Epoch [18/20] Batch 0/118                             Loss D: 0.0158, loss G: 4.7938\n",
      "1250.253406692127\n",
      "Freschet Inception Distance less than 1000\n",
      "Epoch [18/20] Batch 100/118                             Loss D: 0.0047, loss G: 5.3231\n",
      "1291.7126528919284\n",
      "Freschet Inception Distance less than 1000\n",
      "Epoch [19/20] Batch 0/118                             Loss D: 0.0032, loss G: 5.7627\n",
      "1309.1218135639515\n",
      "Freschet Inception Distance less than 1000\n",
      "Epoch [19/20] Batch 100/118                             Loss D: 0.0043, loss G: 5.4708\n",
      "1421.1053134625192\n",
      "Freschet Inception Distance less than 1000\n"
     ]
    }
   ],
   "source": [
    "def process(epochs=NUM_EPOCHS):\n",
    "    step=0\n",
    "    for epoch in range(epochs):\n",
    "        for batch_idx, (real, i) in enumerate(Dataset_Loader):\n",
    "            real = real.to(device)\n",
    "            noise = torch.randn(BATCH_SIZE,Z_DIM,1,1).to(device)\n",
    "            fake = gen(noise).to(device)\n",
    "            ### Train Discriminator max log(D(x)) + log(1 - D(G(z)))\n",
    "            disc_real = disc(real).reshape(-1) # giving proper shape\n",
    "            loss_disc_real =  criterion(disc_real, torch.ones_like(disc_real)) # counting specificly real loss\n",
    "            disc_fake = disc(fake.detach()).reshape(-1) # giving shape\n",
    "            \n",
    "            loss_disc_fake = criterion(disc_fake,torch.zeros_like(disc_fake)) # counting specificly fake loss\n",
    "            loss_disc = (loss_disc_real+loss_disc_fake) / 2 #overall loss\n",
    "            disc.zero_grad() # gradient descent\n",
    "            loss_disc.backward(retain_graph=True)\n",
    "            opt_disc.step()\n",
    "\n",
    "            ### Train Generator min  log(1- D(G(z))) <---> max log(D(G(z)))\n",
    "            output = disc(fake)\n",
    "            loss_gen = criterion(output,torch.ones_like(output))\n",
    "            gen.zero_grad()\n",
    "            loss_gen.backward()\n",
    "            opt_gen.step()\n",
    "\n",
    "            \n",
    "\n",
    "            if batch_idx %100==0:\n",
    "                print(\n",
    "                    f\"Epoch [{epoch}/{epochs}] Batch {batch_idx}/{len(Dataset_Loader)} \\\n",
    "                            Loss D: {loss_disc:.4f}, loss G: {loss_gen:.4f}\"\n",
    "                )\n",
    "                with torch.no_grad():\n",
    "                    fake = gen(fixed_noise).to(device)\n",
    "\n",
    "                    val = calculate_inception_score_and_fid(fake,real)\n",
    "                    print(val)\n",
    "\n",
    "                    print(\"Freschet Inception Distance less than 1000\")\n",
    "\n",
    "                    img_grid_real = torchvision.utils.make_grid(\n",
    "                        real[:32], normalize=True\n",
    "                    )\n",
    "                    img_grid_fake = torchvision.utils.make_grid(\n",
    "                        fake[:32], normalize=True\n",
    "                    )\n",
    "\n",
    "                    writer_fake.add_image(\"Fake\", img_grid_fake,global_step=step)\n",
    "                    writer_real.add_image(\"Real\", img_grid_real,global_step=step)\n",
    "                    \n",
    "                    if val<1200:\n",
    "                        \n",
    "                        ###### Image Saving\n",
    "                        FAKE_MNIST_FOLD = \"E\" + str(epoch) + \"_S\" + str(step) + \"_\"\n",
    "                        FOLDER_PATH = f\"/home/metaphysicist/Coding/Research/Font_Generation/Latest_Results/{FAKE_MNIST_FOLD}\"\n",
    "                        os.mkdir(FOLDER_PATH)\n",
    "                        for i in range(len(fake)):\n",
    "                            save_image(fake[i], f\"{FOLDER_PATH}/fake{i}.png\")\n",
    "                        print(\"fake\",len(fake))\n",
    "                        \n",
    "                        ###### Image Saving\n",
    "                        \n",
    "                    # print(len(fake))\n",
    "                step += 1\n",
    "process(20)\n",
    "# torch.save(gen.state_dict(), 'generator1.pth')\n",
    "# torch.save(disc.state_dict(), 'discriminator1.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# g = Generator(Z_DIM,IMG_CHANNELS,FEATURES_GEN).to(device)\n",
    "# d = Discriminator(IMG_CHANNELS,FEATURES_GEN).to(device)\n",
    "# g.load_state_dict(torch.load('generator.pth'))\n",
    "# d.load_state_dict(torch.load('discriminator.pth'))\n",
    "\n",
    "# # Generate a bunch of fake images\n",
    "# num_samples = 100\n",
    "# latent_samples = torch.randn(BATCH_SIZE,Z_DIM,1,1).to(device)\n",
    "# generated_images = g(latent_samples)\n",
    "\n",
    "# inception_model = torchvision.models.inception_v3(pretrained=True, transform_input=False, aux_logits=True)\n",
    "# inception_model.eval()\n",
    "\n",
    "# # Calculate Inception Scores for each generated image\n",
    "# inception_scores = []\n",
    "\n",
    "# for image in generated_images:\n",
    "    \n",
    "#     image_resized = torch.nn.functional.interpolate(torch.from_numpy(image.cpu()).unsqueeze(0),\n",
    "#                                                     size=(299, 299), mode='bilinear', align_corners=False)\n",
    "#     image_preprocessed = (image_resized - 0.5) / 0.5\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         logits = inception_model(image_preprocessed)\n",
    "\n",
    "#     softmax_probs = torch.softmax(logits, dim=1).numpy()\n",
    "#     kl_divergence = entropy(np.mean(softmax_probs, axis=0), base=2)\n",
    "#     inception_score = np.exp(kl_divergence)\n",
    "\n",
    "#     inception_scores.append(inception_score)\n",
    "\n",
    "# # Print Inception Scores for each generated image\n",
    "# for i, score in enumerate(inception_scores):\n",
    "#     print(\"Inception Score for image\", i + 1, \":\", score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
